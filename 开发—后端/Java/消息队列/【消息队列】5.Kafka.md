# 简介
Linkedln公司。

Apache顶级项目。

1、特点：
* 高吞吐量

* 消息持久化。

* 分布式；

* 消费消息采用pull方式

* 支持online和offOnline场景

# 1、基本概念

1、**Broker**： Kafka集群中的一台或多台服务器。

2、**Topic**：发布到Kafka的每条消息都有一个类别，这个类别就被称为Topic。物理上，不同的Topic是分开存储的；而在逻辑上，虽然一个Topic的消息被保存在一个活着多个Broker上，但用户只需指定消息的Topic即可生产或消费数据，而不必关心数据存在何处。

3、**Partition**：物理上的Topic分区，一个Topic可以分为多个Partition，每个Partition都是一个有序的队列。Parition中的每条消息都会被分配一个有序的ID(offset)；

4、**Consumer**：消息和数据的消费者，可以理解为从Kafka取消息的客户端。

5、**Producer**：消息和数据的生产者，可以理解为Kafka发消息客户端。

6、**Consumer Group（消费者组）** 每个消费者都属于一个特定的消费者组（可为每个消费者指定组名，若不指定组名，则属于默认组）。这是Kafka用来实现一个Topic消息的广播（发送给所有消费者）和单播（发送给任意一个消费者）的手段。一个Topic可以有多个消费者组.topic的消息会被复制（逻辑上的复制）到所有的消费者组中，但每个消费者组只会把消息发送给该组中的一个消费者。如果要实现广播，只要每个消费者都有一个独立的消费者组就可以了。如果要实现单播，只要所有的消费和都在同一个消费者组中就行。使用消费者组还可以对消费者进行自由分组，而不需要多次发送消息到不同的Topic。

# 2、代码实例
https://github.com/apache/kafka/blob/2.1/streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountDemo.java

Kafka作为消息系统的一种，当然可以想起他消息中间件一样的作为消息数据中转的平台，以下以Java语言为例。编写一个使用Kafka系统的实例。

1、安装kafka，为了演示方便，我们使用单机版本的kafka集群。完整版本名称:kafka_2.12-2.1.0

2、引入依赖
```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
    <version>2.1.0</version>
</dependency>

<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-streams -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>2.1.0</version>
</dependency>
```

3、消息生产者
```java
package ch5;


import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.HashMap;



public class MyProducer {
    public static void main(String[] args) {
        // zookeeper url
        String zookeeper = "10.21.1.24:9092";
        // kafka配置
        HashMap<String, Object> props = new HashMap<>();

        // kafka集群地址，9092是kafka服务器默认监听的端口号
        props.put("bootstrap.servers",zookeeper);

        // 消息的序列化/反序列化类型
        props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
        props.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");

        // 连接地址8221
        props.put("zk.connect","10.21.1.24:8121");

        // 话题名称
        String topic = "test";

        // 创建生产者
        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);

        // 发送几条消息
        kafkaProducer.send(new ProducerRecord<String,String>(topic,"key-1","你说谎过几次都瞒不过我"));
        kafkaProducer.send(new ProducerRecord<String,String>(topic,"key-1","你诚实了为何我会难过"));
        kafkaProducer.send(new ProducerRecord<String,String>(topic,"key-1","你究竟拿走了什么"));
        kafkaProducer.send(new ProducerRecord<String,String>(topic,"key-1","让我，寂寞"));

        // 关闭
        kafkaProducer.close();
    }
}

```

4、消息消费者
```java
package ch5;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.Arrays;
import java.util.HashMap;

public class MyConsumer {
    public static void main(String[] args) {
        // zookeeper url
        String zookeeper = "10.21.1.24:9092";
        // kafka配置
        HashMap<String, Object> props = new HashMap<>();

        // kafka集群地址，9092是kafka服务器默认监听的端口号
        props.put("bootstrap.servers",zookeeper);

        // 消息的反序列化类型
        props.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");

        // 消费者分组ID
        props.put("group.id","testGroup1");

        // 自动提交offset
        props.put("enable.auto.commit","true");

        // 提交offset的频率
        props.put("auto.commit.interval.ms","1000");

        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(props);

        // 设置监听topic
        kafkaConsumer.subscribe(Arrays.asList("test"));

        // 轮询
        while(true){
            // 拉取信息，阻塞1s
            ConsumerRecords<String, String> records = kafkaConsumer.poll(Duration.ofSeconds(2));

            if(records.isEmpty()){
                System.out.println("2s内没有收取到信息...");
            }else{
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("分区：%d,offset: %d, key = %s value = %s%n",record.partition(),record.offset(),record.key(),record.value());
                }
            }
        }
    }
}
```


先运行消费者，开启监听，然后开启生产者，写入数据，大致可以在消费者控制台得到如下的输出
```
...
2s内没有收取到信息...
2s内没有收取到信息...
2s内没有收取到信息...
分区：0,offset: 2, key = key-1 value = 你说谎过几次都瞒不过我
分区：0,offset: 3, key = key-1 value = 你诚实了为何我会难过
分区：0,offset: 4, key = key-1 value = 你究竟拿走了什么
分区：0,offset: 5, key = key-1 value = 让我，寂寞
...
```

# 3、Spring Boot整和kafka

见Spring Boot博客，相关章节。


