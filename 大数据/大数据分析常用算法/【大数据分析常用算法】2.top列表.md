# 简介


# Top N设计模式的形式化描述
令N是一个整数，且N大于0.令L是一个List<Tuple2<T,Integer>>，其中T可以是任意类型。
$$
L.size = S;S>N
$$
L的元素为
$$
\{(K_{i},V_{i}),1\le i \le S)\}
$$
其中$K_{i}$的类型为T，$V_{i}$为Integer类型，也可以理解为K的频度。令sort(L)返回已排序的L值，这里使用频度作为键，如下所示:
$$
\{(A_{i},B_{i}),B_{1} \ge B_{2} \ge ... \ge B_{S}\}
$$
其中$(A_{i},B_{i})\in L$，则L的TopN就可以定义如下
$$
topN(L) = \{(A_{i},B_{i}), 1 \le i \le N, B_{1} \ge B_{2} \ge ... \ge B_{N} \ge B_{N + 1} \ge ... \ge B_{S} \}
$$
为了实现TopN，我们需要一个散列表数据结构，从而可以得到键的全序。

同时，TopN还要考虑K在当前集合中唯不唯一的问题，如果不唯一，我们只需使用reduceByKey方法，即可以将同key的数据进行合并。

# 1、Spark普通实现
1、生成随机数据
```java
package com.zhaoyi.topn;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.util.Random;

public class Main {
    public static void main(String[] args) {
        Random r = new Random();
        r.nextInt(1000);
        try{
            writeTopN();
        }catch (Exception e){

        }


    }
    public static void writeTopN() throws IOException {
        File fout = new File("topN1.txt");
        FileOutputStream fos = new FileOutputStream(fout);

        OutputStreamWriter osw = new OutputStreamWriter(fos);
        Random random = new Random(10);
        for (int i = 0; i < 100; i++) {
            osw.write(getStringRandom(5) + "," + random.nextInt(1000) +"\n");
        }
        osw.close();
    }

    //生成随机用户名，数字和字母组成,
    public static String getStringRandom(int length) {

        String val = "";
        Random random = new Random();

        //参数length，表示生成几位随机数
        for(int i = 0; i < length; i++) {

            String charOrNum = random.nextInt(2) % 2 == 0 ? "char" : "num";
            //输出字母还是数字
            if( "char".equalsIgnoreCase(charOrNum) ) {
                //输出是大写字母还是小写字母
                int temp = random.nextInt(2) % 2 == 0 ? 65 : 97;
                val += (char)(random.nextInt(26) + temp);
            } else if( "num".equalsIgnoreCase(charOrNum) ) {
                val += String.valueOf(random.nextInt(10));
            }
        }
        return val;
    }
}
```

2、编写程序
```java
package com.zhaoyi.topn;

import org.apache.commons.collections.IteratorUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.broadcast.Broadcast;
import scala.Tuple2;

import java.util.*;

/***
 * K唯一的Spark实现
 */
public class KSingle {
    public static void main(String[] args) {
        // == 1.获取SC
        SparkConf sparkConf = new SparkConf()
                .setAppName("TopN")
                //.setSparkHome(sparkHome)
                .setMaster("local[*]")
                .set("spark.testing.memory", "2147480000");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);

        // 设置广播变量N
        int n = 10;
        Broadcast<Integer> broadcastTopN = sc.broadcast(n);

        // == 2.加载分析文件
        JavaRDD<String> lines = sc.textFile("topN1.txt");

        // == 3.解析(K,V)
        JavaPairRDD<String, Integer> pairRDD = lines.mapToPair(line -> {
            String[] tokens = line.split(",");
            return new Tuple2<>(tokens[0], Integer.parseInt(tokens[1]));
        });
        /***
         * gD9i0,113
         * P0LpS,380
         * 8Rd46,293
         * 34b3C,290
         * 12I7u,246
         * ...
         */
        // == 4.各个分区排序 本地topN
        JavaRDD<SortedMap<Integer, String>> partitions = pairRDD.mapPartitions(iter -> {
            // 使用有序列表筛选topN
            // setup
            SortedMap<Integer, String> localTopN = new TreeMap<Integer, String>();
            final int N = broadcastTopN.value();
            // map
            while (iter.hasNext()) {
                Tuple2<String, Integer> t = iter.next();
                // 以V（t._2），即次数作为Map的K
                localTopN.put(t._2, t._1);
                // 只保留TopN
                if (localTopN.size() > N) {
                    // 删除频度最小的元素，如果这里是bottomN,只需删除最后一个元素就可以，即：topN.remove(topN.lastKey()
                    localTopN.remove(localTopN.firstKey());
                }
            }
            // cleanup
            return Collections.singletonList(localTopN).iterator();
        });

        // == 最终topN
        SortedMap<Integer, String> reduceResult = partitions.reduce((m1, m2) -> {
            // 将m1,m2合并到一个topN列表
            SortedMap<Integer, String> topN = new TreeMap<>();
            final int N = broadcastTopN.value();
            // 处理m1
            for (Map.Entry<Integer, String> entry : m1.entrySet()) {
                topN.put(entry.getKey(), entry.getValue());
                // 只保留TopN
                if (topN.size() > N) {
                    // 删除频度最小的元素，如果这里是bottomN,只需删除最后一个元素就可以，即：topN.remove(topN.lastKey()
                    topN.remove(topN.firstKey());
                }
            }

            // 处理m2
            for (Map.Entry<Integer, String> entry : m2.entrySet()) {
                topN.put(entry.getKey(), entry.getValue());
                // 只保留TopN
                if (topN.size() > N) {
                    topN.remove(topN.firstKey());
                }
            }
            return topN;
        });

        System.out.println("### top N ###");

        for (Map.Entry<Integer, String> entry : reduceResult.entrySet()) {
            System.out.println(entry.getValue() + "," + entry.getKey());
        }
        /**
         * ### top N ###
         * H030R,902
         * iH92W,907
         * 17lCB,928
         * 09fJF,953
         * 6BpSX,959
         * uF96x,960
         * 651qd,971
         * eX5bl,974
         * YoE8H,979
         * 52b0o,981
         */
    }

}
```

# 2、Spark takeOrdered
Spark提供一个更快捷的方法完成上面的操作
```java
package com.zhaoyi.topn;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.broadcast.Broadcast;
import scala.Tuple2;

import java.util.*;

public class takeOrderedMethod {
    public static void main(String[] args) {

        // == 1.获取SC
        SparkConf sparkConf = new SparkConf()
                .setAppName("TopN")
                //.setSparkHome(sparkHome)
                .setMaster("local[*]")
                .set("spark.testing.memory", "2147480000");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);

        // N
        int n = 10;

        // == 2.加载分析文件
        JavaRDD<String> lines = sc.textFile("topN1.txt");

        // == 3.解析(K,V)
        JavaPairRDD<String, Integer> pairRDD = lines.mapToPair(line -> {
            String[] tokens = line.split(",");
            return new Tuple2<>(tokens[0], Integer.parseInt(tokens[1]));
        });

        List<Tuple2<String, Integer>> topN = pairRDD.takeOrdered(n, new MyTopNComparator());
        System.out.println("### top N ###");
        System.out.println(topN);
        /**
         * ### top N ###
         ### top N ###
         [(52b0o,981), (YoE8H,979), (eX5bl,974), (651qd,971), (uF96x,960), (6BpSX,959), (09fJF,953), (17lCB,928), (iH92W,907), (6RE79,902)]
         */

    }
}
```



