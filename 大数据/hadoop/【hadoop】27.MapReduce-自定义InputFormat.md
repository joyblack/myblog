# ç®€ä»‹
æ— è®ºhdfsè¿˜æ˜¯mapreduceï¼Œå¯¹äºå°æ–‡ä»¶éƒ½æœ‰æŸæ•ˆç‡ï¼Œå®è·µä¸­ï¼Œåˆéš¾å…é¢ä¸´å¤„ç†å¤§é‡å°æ–‡ä»¶çš„åœºæ™¯ï¼Œæ­¤æ—¶ï¼Œå°±éœ€è¦æœ‰ç›¸åº”è§£å†³æ–¹æ¡ˆã€‚å°†å¤šä¸ªå°æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶SequenceFileï¼ŒSequenceFileé‡Œé¢å­˜å‚¨ç€å¤šä¸ªæ–‡ä»¶ï¼Œå­˜å‚¨çš„å½¢å¼ä¸ºâ€œæ–‡ä»¶è·¯å¾„+åç§°â€ä¸ºkeyï¼Œæ–‡ä»¶å†…å®¹ä¸ºvalueã€‚

å°æ–‡ä»¶çš„ä¼˜åŒ–æ— éä»¥ä¸‹å‡ ç§æ–¹å¼ï¼š
* åœ¨æ•°æ®é‡‡é›†çš„æ—¶å€™ï¼Œå°±å°†å°æ–‡ä»¶æˆ–å°æ‰¹æ•°æ®åˆæˆå¤§æ–‡ä»¶å†ä¸Šä¼ HDFS
* åœ¨ä¸šåŠ¡å¤„ç†ä¹‹å‰ï¼Œåœ¨HDFSä¸Šä½¿ç”¨mapreduceç¨‹åºå¯¹å°æ–‡ä»¶è¿›è¡Œåˆå¹¶
* åœ¨mapreduceå¤„ç†æ—¶ï¼Œå¯é‡‡ç”¨CombineTextInputFormatæé«˜æ•ˆç‡

æœ¬èŠ‚æˆ‘ä»¬å®ç°ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼š
* è‡ªå®šä¹‰ä¸€ä¸ªInputFormat
* æ”¹å†™RecordReaderï¼Œå®ç°ä¸€æ¬¡è¯»å–ä¸€ä¸ªå®Œæ•´æ–‡ä»¶å°è£…ä¸ºKV
* åœ¨è¾“å‡ºæ—¶ä½¿ç”¨SequenceFileOutPutFormatè¾“å‡ºåˆå¹¶æ–‡ä»¶

## 1ã€æ¡ˆä¾‹
## 1.1ã€è¾“å…¥æ•°æ®
æˆ‘ä»¬æ¥å¤„ç†3ä¸ªæ–‡ä»¶
D:/a.txt
```
a poor guy.
```

D:/b.txt
```
boss will come here, hurry up.
```

D:/c.txt
```
can I sit there?
```

æœ€ç»ˆç”Ÿæˆä¸€ä¸ªSequenceFileæ–‡ä»¶ï¼Œå†…å®¹ä¸é€‚åˆæµè§ˆï¼ˆæœ‰ä¹±ç ï¼‰ã€‚

## 2ã€ç¼–å†™ä»£ç 
ä»£ç è§æ¨¡å—smallfileã€‚

1ã€è‡ªå®šä¹‰InputFormatï¼Œè¯¥Formatéœ€è¦ä¸€ä¸ªReaderï¼Œæˆ‘ä»¬ç´§æ¥ç€å®šä¹‰ã€‚
```java
package com.zhaoyi.smallfile;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;


import java.io.IOException;

public class MyInputFormat extends FileInputFormat<NullWritable, BytesWritable> {
    // ä»»ä½•å¤§å°éƒ½ä¸åˆ†ç‰‡
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
        return false;
    }

    @Override
    public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
        MyRecordReader reader = new MyRecordReader();
        // åˆå§‹åŒ–reader
        reader.initialize(inputSplit, taskAttemptContext);

        return reader;
    }
}

```

2ã€è‡ªå®šä¹‰RecordReaderç±»
```java
package com.zhaoyi.smallfile;

import javafx.util.converter.DateStringConverter;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;
import java.util.Date;
import java.util.UUID;

public class MyRecordReader extends RecordReader<NullWritable, BytesWritable> {
    private UUID uuid = UUID.randomUUID();
    Boolean more = true;// æ˜¯å¦è¿˜æœ‰æ›´å¤šçš„æ•°æ®éœ€è¦è¯»
    BytesWritable value = new BytesWritable();// è¾“å‡ºå€¼
    FileSplit split;// åˆ†ç‰‡ä¿¡æ¯
    private Configuration configuration;// é…ç½®ä¿¡æ¯
    public MyRecordReader() {
        super();
    }


    // åˆå§‹åŒ–ä¿¡æ¯
    @Override
    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
        System.out.println("uuid = " + uuid);
        System.out.println("########initialize#######");
        split = (FileSplit)inputSplit;
        configuration = taskAttemptContext.getConfiguration();
    }


    @Override
    public boolean nextKeyValue() throws IOException, InterruptedException {
        System.out.println("#######nextKeyValue###########");
        System.out.println(split.getPath());
        if(more){
            System.out.println("this is more ...");
            // å®šä¹‰ç¼“å­˜
            byte[] contents = new byte[(int)split.getLength()];
            // è·å–æ–‡ä»¶ç³»ç»Ÿ
            FileSystem fs  = FileSystem.get(this.configuration);
            // è¯»å–æ–‡ä»¶å†…å®¹
            FSDataInputStream fsDataInputStream = fs.open(split.getPath());
            // è¯»å…¥ç¼“å†²åŒº
            IOUtils.readFully(fsDataInputStream,contents,0,contents.length);
            // è¾“å‡ºåˆ°value
            value.set(contents, 0, contents.length);

            more = false;// å·²ç»å¤„ç†å®Œ
            return true; //è¿”å›trueè¡¨ç¤ºè¿˜ä¼šè¿­ä»£è¯¥æ–¹æ³•ï¼ˆä¸‹ä¸€æ¬¡ä¸ä¼šè¿›å…¥è¿™é‡Œå¾…å¤„ç†é€»è¾‘ï¼Œmoreï¼‰ï¼Œè¿™åœ¨ä¸€æ¬¡æ²¡å¤„ç†å®Œæ—¶å¾ˆæœ‰ç”¨ï¼Œè™½ç„¶æˆ‘ä»¬è¿™é‡Œå¹¶æ— æ„ä¹‰ã€‚
        }
        return  false;
    }

    @Override
    public NullWritable getCurrentKey() throws IOException, InterruptedException {
        return NullWritable.get();
    }

    @Override
    public BytesWritable getCurrentValue() throws IOException, InterruptedException {
        return value;
    }

    @Override
    public float getProgress() throws IOException, InterruptedException {
        return 0;
    }

    @Override
    public void close() throws IOException {

    }
}
```

3ã€Mapper
```java
package com.zhaoyi.smallfile;

import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

public class MyMapper extends Mapper<NullWritable, BytesWritable, Text, BytesWritable> {
    Text k = new Text();
    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        FileSplit inputSplit = (FileSplit) context.getInputSplit();
        // æ–‡ä»¶è·¯å¾„ä½œä¸ºkey
        k.set(inputSplit.getPath().toString());
    }

    @Override
    protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException {
        context.write(k, value);
    }
}
```

4ã€Driver
``` java
package com.zhaoyi.smallfile;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;

import java.io.IOException;

public class MyDriver {
    public static void main(String[] args) throws Exception {
        Job job = Job.getInstance(new Configuration());

        job.setJarByClass(MyDriver.class);


        job.setOutputValueClass(BytesWritable.class);
        job.setOutputKeyClass(Text.class);

        // å…³è”çš„è‡ªå®šä¹‰InputFormat
        job.setInputFormatClass(MyInputFormat.class);
        // è®¾ç½®è¾“å‡ºæ–‡ä»¶çš„æ ¼å¼ä¸ºsequenceFile
        job.setOutputFormatClass(SequenceFileOutputFormat.class);

        job.setMapperClass(MyMapper.class);

        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        System.exit(job.waitForCompletion(true)? 1:0);
    }
}

```

æœ€ç»ˆè¾“å‡ºå¦‚ä¸‹ï¼š
```
SEQorg.apache.hadoop.io.Text"org.apache.hadoop.io.BytesWritable      _Yç’?åŠ€å‡?>ZE   +   file:/D:/hadoop/input/a.txt   a poor guy.   >   file:/D:/hadoop/input/b.txt   boss will come here, hurry up.   0   file:/D:/hadoop/input/c.txt   can I sit there?
```


